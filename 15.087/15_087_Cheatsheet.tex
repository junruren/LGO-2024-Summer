\documentclass[10pt,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{listings}
\usepackage{amsmath,amsthm,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{color,graphicx,overpic}
\usepackage{hyperref}
\usepackage[dvipsnames]{xcolor}
\usepackage[T1]{fontenc}

\usepackage{MnSymbol}
\usepackage{graphicx}
\usepackage{wrapfig}

% This sets page margins to .1 inch if using letter paper, and to 1cm
% if using A4 paper. (This probably isn't strictly necessary.)
% If using another size paper, use default 1cm margins.
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=0.2in,left=0.2in,right=0.2in,bottom=0.2in} }
    {\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
        {\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
    }

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother

% Itemize to use less space
\usepackage{enumitem}
\setlist{leftmargin=*, nosep}
\setenumerate{nosep}

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

%My Environments
\newtheorem{example}[section]{Example}

\newcommand{\Blue}[1]{\noindent{\textbf{\textcolor{Blue}{#1 -}}}}
\newcommand{\Red}[1]{\noindent{\textbf{\textcolor{BrickRed}{#1 -}}}}
\newcommand{\Green}[1]{\noindent{\textbf{\textcolor{PineGreen}{#1 -}}}}
\newcommand{\Hint}[1]{\noindent{\textcolor{Orange}{#1}}}

\lstset{breaklines=true}
\lstset{upquote=true}
% -----------------------------------------------------------------------

\begin{document}
\raggedright
\footnotesize
\begin{multicols}{4}


% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\Hint{Remember the T-Rex: Go beyond the summary statistics!}

\includegraphics[width=0.20\textwidth]{trex.jpg}

\section{Fitting Distributions}

\begin{enumerate}
    \item Collect data whose patterns are believed similar to the distribution of the random variable of interest.
    \item Postulate one (or more) candidate probability distributions (e.g., normal (R: \texttt{"norm"}), lognormal (R: \texttt{"lnorm"}), uniform (R: \texttt{"unif"})\dots)
    \item Fit the postulated distribution to the data: finding values of the parameters that best fit the data. Two methods: Moment Matching (R: \texttt{"mme"}) and Maximum Likelihood Estimation Method (R: \texttt{"mle"})\begin{lstlisting}[language=R]
model <- fitdist(df, "norm", method="mme")\end{lstlisting}
    \item Evaluate candidate distributions using goodness-of-fit. E.g., \Green{Akaike Information Criterion (AIC)} smaller the better.
    \item Tweak the distributions if necessary to account for any data limitations rendering the past not quite representative of the future.
\end{enumerate}

\section{Linear Regression}

Linear regression means the dependent variable is \Hint{linear in the model coefficients}.

\begin{itemize}
    \item Linear: $y = b_0 + b_1 x_1 + b_2 x_2$
    \item Not linear: $y = b_0 x_1^{b_1} + b_3 x_2^{b_2}$
\end{itemize}

\subsection{\texttt{lm} in R}

Rentals as a function of temperature and humidity:

\begin{lstlisting}[language=R]
mod <- lm(data = df, rentals ~ temp + rel_humidity)
summary(mod)
\end{lstlisting}

\subsection{Consider a ``Best Fitting'' Line}
\begin{displaymath}
    \boxed{
        \hat{y} = b_0 + b_1 x
    }
\end{displaymath}

\Blue{Slope $b_1$} sign is same as the sign of $\text{CORR}(X,Y)$. $\text{CORR}(X,Y)$ is between $[-1,1]$ and unit-less.
But $b_1$ has units.

\begin{displaymath}
    b_1 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n} (x_i - \bar{x})^2} = \text{CORR}(X,Y) \cdot \frac{\text{SD}(Y)}{\text{SD}(X)}
\end{displaymath}

\Blue{Intercept} $b_0 = \bar{y} - b_1 \bar{x}$

\Red{Mean-center data} if both $y$ and $x$ variables are mean-centered, rerun linear regression to get:

\begin{displaymath}
    \hat{y}_i - \bar{y} = b_1 (x_i - \bar{x})
\end{displaymath}

\begin{itemize}
    \item new intercept will be $0$
    \item new slope remains $b_1$
\end{itemize}

\Green{Regression to the mean} If $x = \bar{x}$ (the average of all $x$ values), the predicted $\hat{y} = \bar{y}$
(average of all $y$ values), \Hint{independent from $x$}.

\subsection{Independent Variables}

\Red{Categorical variables (``factor variable'' in R)} if encoded with one-hot, one category should be dropped to avoid perfect multicollinearity
(a situation where one predictor variable can be perfectly predicted from the others). This omitted category serves as a
reference category against which the other categories are compared. This approach is known as creating "dummy
variables."

\subsection{Evaluation}

\Blue{Residuals (error)} $e_i = y_i - \hat{y}_i$

\Blue{Sum of Squared Residuals}
\begin{displaymath}
SSR = \sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\end{displaymath}

\Blue{Total Sum of Squares} a measure of the total variability in the observed data.
\begin{displaymath}
SST = \sum_{i=1}^{n} (y_i - \bar{y}_i)^2
\end{displaymath}

\Red{$R^2$} the \underline{proportion reduction} in sum of squared residuals by the regression model compared to the
baseline model (which always predicts the average value of all $y$s in the data: $\hat{y} = \bar{y}$).

\begin{displaymath}
    \boxed{
        R^2 = 1 - \frac{SSR}{SST} = \frac{SST - SSR}{SST}
    }
\end{displaymath}

\begin{itemize}
    \item $R^2$ of a regression model lies \Hint{between 0 and 1}.
    \item Adding a new independent variable to a regression model \Hint{can only increase $R^2$}.
    \item $R^2$ on its own cannot judge how good a model is.
\end{itemize}

\Green{$R^2$ in multiple linear regression} equals the
\Hint{square of the correlation between the \underline{actual values $y$} and the \underline{predicted values $\hat{y}$}}.
\begin{displaymath}
    R^2 = \left(\text{CORR}(y, \hat{y})\right)^2
\end{displaymath}
In the case of a perfect fit where every prediction of $y_i$ is going to be correct, then $R^2 = 1$

\Green{$R^2$ in simple linear regression} equals the
\Hint{square of correlation between dependent variable $y$ and independent variable $x$}.
\begin{displaymath}
    R^2 = \left(\text{CORR}(y, x)\right)^2
\end{displaymath}

\Blue{Degrees of Freedom} the number of observations minus the number of parameters estimated (including the intercept): $df = n - p - 1$

\Blue{Residual Variance} $\frac{SSR}{df}$

\Blue{Residual Standard Error} provides a measure of the average distance that the observed values fall from the
regression line.
\begin{displaymath}
RSE = \sqrt{\frac{SSR}{df}}
\end{displaymath}
\begin{itemize}
    \item Smaller RSE: the model's predictions are closer to the actual data points, suggesting a good fit.
    \item Larger RSE: the model's predictions are further from the actual data points, suggesting a poor fit.
\end{itemize}

\subsection{Troubleshooting}

The model's ability to predict the future and its interpretability can be impaired by

\begin{itemize}
    \item The presence of \underline{irrelevant} independent variables
    \item The presence of \underline{highly correlated} independent variables
    \item The presence of \underline{``too many''} variables relative to the size of the dataset
\end{itemize}

\Blue{\underline{Irrelevant} variables} have large p-values. Smaller p-value (R: \texttt{Pr(>|t|)}), the better. If
\Hint{lower than 0.05}, consider it \Hint{significant} at the 5\% significance level; otherwise, consider it
non-significant at the 5\% significance level. In R, a variable with \Hint{one or more \texttt{*} is considered
significant at 5\% level. A variable not significant at the 5\% level but at the 10\% level can be stated as ``less
clear''.}

\Green{p-value of a variable $x_i$} the probability of observing a coefficient estimate as extreme, or more extreme,
than the one actually obtained by the regression run in a similar sample \underline{assuming} $\beta_i = 0$.

\Blue{\underline{Highly correlated} variables} Detected by inspecting the correlation matrix. (R: \texttt{cor(df)}).

\Green{Multicollinearity} When two variables are highly correlated (typically correlations \Hint{higher than 0.75} in
magnitude), resolve this by removing either of the independent
variables and \underline{running the linear regression \textbf{again}}.

\Blue{Overfitting (\underline{``too many''} variables)} leads to poorly predicting future data. Remedy by (1) avoiding
``non-significant'' variables uising the starts \texttt{*} as a guide and (2) out-of-sample testing.

\Green{Out-of-Sample Testing}
\begin{enumerate}
    \item Partition data set into 70\% trainning set and 30\% test set before creating the regression model.
    \item Generate a set of models (e.g. with different sets of variables) using \underline{only} the training data.
    \item Evaluate models on the \underline{test set} using \Hint{out-of-sample $R^2$}.
\end{enumerate}

\Green{Out-of-Sample $R^2$}
\begin{displaymath}
\boxed{
\begin{split}
        &\text{OSR}^2 = \\
        &1 - \frac{\text{SSR of regression model applied to the test set}}{\text{SSR of baseline model applied to the test set}}
\end{split}
}
\end{displaymath}

\includegraphics[width=0.25\textwidth]{overfitting.jpg}

\Red{Key takeaways}

\begin{itemize}
    \item Choose final model based on \underline{out-of-sample} predictive quality metrics.
    \item Use metrics like $R^2$ and the standard error of regression \underline{in combination} because they have different strength and weaknesses.
    \item Only include significant variables that are not highly correlated.
    \item Coefficients should ``make sense''.
    \item Use for interpolation rather than extrapolation.
\end{itemize}

\Blue{Inferring model's coefficient} from the practice: ``Correlation is not the same as causation. It could be that
individuals with other contributing factors happen to sleep longer. It could be that these other factors, not sleep per
se, are causally linked to the condition under study.''

\Red{Technical caution} for the p-value to be correct, the ``unaccounted for'' differences in the regression model
$\epsilon$ (think of the residuals) needs to have zero mean, constant standard deviation, be independent and follow a normal distribution.

\Green{Linearity} there is no curvature
\Green{Homoskedasticity} the dispersion of $e_i = y_i - \hat{y}_i$ is not systematically smaller or larger for large
$x_i$ values compared to small $x_i$ values.
\includegraphics[width=0.25\textwidth]{residuals1.jpg}
Top-right: hetero. Bottom-right:homo.
\Green{Normality} the residuals are approximately normal.
\includegraphics[width=0.25\textwidth]{residuals2.jpg}

\section{Clustering}

\Blue{$k$-means} with initial centroids $\mu_1,\dots,\mu_K \in \mathbb{R}^n$ (usually randomly selected),
repeat until convergence:
assign each point $i=1,\dots,n$ \fbox{$z_i \leftarrow \arg \min_{k=1,\dots,K}||\phi(s_i) - \mu_k||^2$}
and then recenter each cluster $k=1,\dots,K$
\fbox{$\mu_k \leftarrow \frac{\text{all points in this cluster sumed}}{\text{\# points in this cluster}}$}.
$k$-means is guaranteed to converge to a local (not global) optima even with
random initialization of centroids. Solution: run multiple times with different
random init or init with heuristic.

\Green{$k$-means objective func}
\fbox{$\text{L}_{k\text{-means}}(z,\mu) = \sum_{i=1}^{n}||\phi(x_i) - \mu_{z_i}||^2$}

\Green{Works the best} when clusters have ``round shape''.

\begin{lstlisting}[language=R]
set.seed(123)
# k = 2
# nstart = 100 is usually a good choice
my_clustering <- kmeans(df, centers=2, nstart=100)
my_clustering$centers # Show centers
df <- mutate(df, cluster=my_clustering$cluster) # Label the cluster assignments
\end{lstlisting}

\Blue{Scree Plot} to help choosing $k$ value by displaying the tradeoff between number of clusters $k$ and the
within-cluster sum of squared distances. \Hint{As $k$ increases, this value should decrease.}
\includegraphics[width=0.24\textwidth]{scree.jpg}
If SSD decreases very slowly as we add more clusters, this dataset isn't cluster-able.

\section{Classification}

\Red{Why not Linear Regression?} (1) Predicted $y$ can be negative or more than 1.0; (2) hard to interpret coefficients
and hard to assess variable significance using \texttt{*}s.

\Red{Logistic Regression} a sigmoid function to help.
\includegraphics[width=0.24\textwidth]{sigmoid.jpg}

\subsection{\texttt{glm} (generalized linear model) in R}

\begin{lstlisting}[language=R]
mod <- glm(data = train, default ~ ., family="binomial")
\end{lstlisting}

Binary classification, not multi-class.

\subsection{Confusion Matrix}
\includegraphics[width=0.24\textwidth]{confusion_matrix.jpg}

Usually a trained classification model can be compared with a baseline model that always predicts the most common
outcome.

Accuracy/error rate isn't enough: as long as TP and TN adds to the same number, accuracy won't change assuming that all
errors are equally important. To mitigate, add in financial considerations.

\Green{True Positive Rate} i.e., sensitivity or \textbf{recall}. \fbox{$\text{TPR} = \frac{TP}{TP + FN}$} $\text{TPR} + \text{FNR} = 1$

\Green{True Negative Rate} i.e., specificity. \fbox{$\text{TNR} = \frac{TN}{TN + FP}$} $\text{TNR} + \text{FPR} = 1$

\begin{itemize}
    \item TPR and FPR cannot be chosen independently.
    \item If a cutoff chosen improves FPR, TPR will be adversely affected and vice-versa.
\end{itemize}

\Red{ROC Curve} receiver operating characteristic, captures the trade-off between FPR and TPR as the cutoff is varied.
\Hint{\begin{enumerate}
    \item Identify the more expensive type of error (FN or FP)
    \item Decide on the max you are willing to tolerate that type of error
    \item Use the ROC curve to pick the cutoff
\end{enumerate}}

\Red{Area Under the ROC Curve} in general, models with \Hint{higher AUC are preferable}.

\section{CART}

\begin{enumerate}
    \item For each independent variable and each possible way to split the dataset based on that variable, calculate the impurity (reduction in overall SSR)
    \item Choose the split (\Hint{independent variable to split on + the cutoff value to split at}) that has the max reduction in SSR and split the dataset into two accordingly
    \item Repeat
\end{enumerate}

\Hint{CART models can be used for both a categorical dependent variable and continuous dependent variable.}

\begin{lstlisting}[language=R]
mdl <- rpart(data=train, default ~ installment + fico_score, minbucket=value, cp=value)
prp(mdl) # Visualize
predict(mdl, newdata=test)
\end{lstlisting}

\Red{\texttt{minsplit}} the minimum number of observations that must exist in a node in order for a split to be attempted.

\Red{\texttt{minbucket}} the minimum number of observations in any terminal node. If only one of \texttt{minbucket} or
\texttt{minsplit} is specified, the code either sets \texttt{minsplit} to \texttt{minbucket*3} or \texttt{minbucket} to
\texttt{minsplit/3}, as appropriate. \Hint{Smaller \texttt{minbucket}, more splits in the final tree.}

\Red{\texttt{cp} complexity parameter} Any split that does not decrease the overall lack of fit by a factor of cp is not attempted. For instance, with anova splitting, this means that the overall R-
squared must increase by cp at each step. The main role of this parameter is to save computing time by pruning off
splits that are obviously not worthwhile. Essentially, the user informs the program that any split which does not
improve the fit by cp is not of interest, and that hence the program need not pursue it
\Hint{Smaller \texttt{cp}, more layers (more underlying splits in the final tree).}

\Green{Advantages of CART}
\begin{itemize}
    \item Can discover interactions between variables without explicitly incorporating interaction terms
    \item Can learn nonlinear patterns
    \item May mirror human decision making processes
    \item Decision Rule is highly transparent: A series of if/else statements.Even simpler than linear regression. Can be explained to non-experts
\end{itemize}

\Red{Limiations of CART}
\begin{itemize}
    \item CART predictions lack smoothness
    \item CART models may exhibit instability
    \item May not lead to the strongest out-of-sample predictive performance
\end{itemize}

\Hint{Exam Cautions}: Watch out what the leaf nodes represent and what the question asks for: could be two opposite.

\subsection{Cross-Validation}
\Blue{Goal} finding the values of \texttt{minbucket} and \texttt{cp}.
k-fold Cross-validation:
\includegraphics[width=0.25\textwidth]{kfold.jpg}
\texttt{minbucket} is usually fixed throughout the cross-validation.

\section{Side bar}

\includegraphics[width=0.25\textwidth]{normal_distribution_property.jpg}

\begin{itemize}
    \item about 68\% of the total values lie within 1 standard deviation of the mean. 
    \item about 95\% lie within 2 standard deviations of the mean
    \item about 99.7\% lie within 3 standard deviations of the mean.
\end{itemize}

\end{multicols}
\end{document}